{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrkZUs9TyQ5Ln+ixhiaoCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppujari/PyTorch/blob/main/PyTorch_tips.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-15"
      ],
      "metadata": {
        "id": "Qi2Bnb-CA5Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to reduce data loading bottlenecks especially with large batches or high-resolution images.Use torch.utils.data.DataLoader with pin_memory=True and non_blocking=True for faster GPU transfers.\n",
        "When moving data to GPU, most people just do:\n",
        "data = data.to(device)\n",
        "Pinned memory allows faster transfers between CPU and GPU because it uses page-locked memory that the GPU can access directly via DMA (Direct Memory Access). The non_blocking=True allows the transfer to happen asynchronously, so CPU computation can overlap with the memory transfer."
      ],
      "metadata": {
        "id": "iCQ47_SGBA7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you can speed this up significantly by:\n",
        "\n",
        "Setting pin_memory=True in your DataLoader:"
      ],
      "metadata": {
        "id": "v8z3fbltB5Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, pin_memory=True)"
      ],
      "metadata": {
        "id": "0td5x59vAz0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using non_blocking=True when transferring:"
      ],
      "metadata": {
        "id": "iw7pc0pWCREn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.to(device, non_blocking=True)\n",
        "target = target.to(device, non_blocking=True)"
      ],
      "metadata": {
        "id": "Ighri_2wCVwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Custom dataset for demonstration\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size=1000, input_dim=784, num_classes=10):\n",
        "        self.size = size\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate random data and labels\n",
        "        data = torch.randn(self.input_dim)\n",
        "        target = torch.randint(0, self.num_classes, (1,)).squeeze()\n",
        "        return data, target\n",
        "\n",
        "# Simple neural network\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=512, num_classes=10):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_without_optimization(model, dataloader, criterion, optimizer, device, epochs=1):\n",
        "    \"\"\"Training loop WITHOUT DMA optimization\"\"\"\n",
        "    print(\"Training WITHOUT DMA optimization...\")\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            # Standard (slow) way - blocking transfers\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "    return end_time - start_time\n",
        "\n",
        "def train_with_optimization(model, dataloader, criterion, optimizer, device, epochs=1):\n",
        "    \"\"\"Training loop WITH DMA optimization\"\"\"\n",
        "    print(\"Training WITH DMA optimization...\")\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            # Optimized way - non-blocking transfers with pinned memory\n",
        "            data = data.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "\n",
        "            # CPU can do other work here while GPU transfer happens\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # By now, the data transfer is likely complete\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "    return end_time - start_time\n",
        "\n",
        "def main():\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device.type == 'cpu':\n",
        "        print(\"Note: Running on CPU. DMA optimization benefits are most visible with GPU.\")\n",
        "\n",
        "    # Create dataset and dataloaders\n",
        "    dataset = DummyDataset(size=2000, input_dim=784)\n",
        "\n",
        "    # DataLoader WITHOUT pinned memory\n",
        "    dataloader_normal = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=False  # Standard approach\n",
        "    )\n",
        "\n",
        "    # DataLoader WITH pinned memory (for DMA optimization)\n",
        "    dataloader_optimized = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True  # Enables faster GPU transfers\n",
        "    )\n",
        "\n",
        "    # Create models (separate instances for fair comparison)\n",
        "    model1 = SimpleNet().to(device)\n",
        "    model2 = SimpleNet().to(device)\n",
        "\n",
        "    # Copy weights to make models identical\n",
        "    model2.load_state_dict(model1.state_dict())\n",
        "\n",
        "    # Setup optimizers and loss\n",
        "    optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "    optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"PERFORMANCE COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Train without optimization\n",
        "    time_normal = train_without_optimization(\n",
        "        model1, dataloader_normal, criterion, optimizer1, device, epochs=2\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Train with optimization\n",
        "    time_optimized = train_with_optimization(\n",
        "        model2, dataloader_optimized, criterion, optimizer2, device, epochs=2\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Normal training time: {time_normal:.2f} seconds\")\n",
        "    print(f\"Optimized training time: {time_optimized:.2f} seconds\")\n",
        "\n",
        "    if time_normal > time_optimized:\n",
        "        speedup = (time_normal - time_optimized) / time_normal * 100\n",
        "        print(f\"Speedup: {speedup:.1f}% faster with DMA optimization\")\n",
        "    else:\n",
        "        print(\"Note: Speedup may not be visible with small datasets or on CPU\")\n",
        "\n",
        "    print(\"\\nKey optimizations used:\")\n",
        "    print(\"1. pin_memory=True in DataLoader\")\n",
        "    print(\"2. non_blocking=True in .to(device) calls\")\n",
        "    print(\"3. Overlapping CPU work with GPU memory transfers\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E9GSG4l7DmbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-14"
      ],
      "metadata": {
        "id": "9zW2ygc0kbNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize GPU Memory Usage  \n",
        "Tracking memory usage helps identify inefficiencies, spikes, and fragmentation in GPU memory.\n",
        "\n",
        "*   Detects memory spikes and fragmentation.\n",
        "*   Optimizes model scaling and deployment.\n",
        "\n",
        "Use this when developing memory-intensive models, deploying on limited-resource hardware, or scaling across multiple GPUs.\n",
        "This generates a profile.pkl file, storing detailed memory usage data. Visualize it using PyTorch's [memory visualizer](https://docs.pytorch.org/memory_viz)."
      ],
      "metadata": {
        "id": "quWtfzSQkw1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "50R3ns3dmQpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start recording memory snapshot history\n",
        "torch.cuda.memory._record_memory_history(max_entries=100000)\n",
        "\n",
        "# Example model and computation\n",
        "model = nn.Linear(10_000, 50_000, device=\"cuda\")\n",
        "for _ in range(3):\n",
        "    inputs = torch.randn(5_000, 10_000, device=\"cuda\")\n",
        "    outputs = model(inputs)\n",
        "\n",
        "# Dump memory history to a file and stop recording\n",
        "torch.cuda.memory._dump_snapshot(\"profile.pkl\")\n",
        "torch.cuda.memory._record_memory_history(enabled=None)\n"
      ],
      "metadata": {
        "id": "zt8dmF_EkjHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-13"
      ],
      "metadata": {
        "id": "zSjT4Zr3PAx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use torch.cuda.amp for Automatic Mixed Precision (AMP)**"
      ],
      "metadata": {
        "id": "wYSTgVZKPRmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Mixed Precision (AMP) allows you to train your models using a mix of float16 and float32 data types. This can significantly speed up training and reduce memory usage, especially on modern GPUs that support Tensor Cores."
      ],
      "metadata": {
        "id": "lT-a2bAvPoeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "oq_3OIm3PvKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple dataset\n",
        "input_data = torch.randn(100, 10)  # 100 samples, 10 features each\n",
        "targets = torch.randint(0, 2, (100,))  # 100 samples, binary targets"
      ],
      "metadata": {
        "id": "1XMM3eVHQy8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader\n",
        "dataset = TensorDataset(input_data, targets)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "OXwm6rrVQ1NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define your model, loss function, and optimizer:\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 2)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "zmgy-XioPhz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use autocast and GradScaler in your training loop:\n",
        "# Initialize the GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "1ctqpddvQGs5",
        "outputId": "da02b104-2ac1-4535-8c71-3acb3c931821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-2004642073>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-5-2004642073>:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.825927734375\n",
            "Epoch 2/5, Loss: 0.7431640625\n",
            "Epoch 3/5, Loss: 0.857177734375\n",
            "Epoch 4/5, Loss: 0.740234375\n",
            "Epoch 5/5, Loss: 0.687744140625\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-12"
      ],
      "metadata": {
        "id": "tzoLVgvOuSxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use gradient checkpointing to reduce memory usage for large models.\n",
        "\n",
        "This is important for training very large models that don't fit in GPU memory. So, instead of storing all activations for the backward pass, this strategy saves only checkpoints and recomputes activations during backprop. It can reduce memory usage by up to 80%. It's useful for:\n",
        "\n",
        "*   Transformer models with many layers\n",
        "*   Deep ResNets\n",
        "*   Any model where you're hitting OOM errors etc"
      ],
      "metadata": {
        "id": "WNgLGNdLuY5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_and_measure(model, use_checkpoint):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Track memory usage\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(10):\n",
        "        # Generate random batch\n",
        "        x = torch.randn(32, 100)\n",
        "        y = torch.randn(32, 100)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(x, use_checkpoint=use_checkpoint)\n",
        "        loss = nn.MSELoss()(output, y)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
        "\n",
        "    return end_time - start_time, peak_memory\n"
      ],
      "metadata": {
        "id": "wP-4mFoHw2VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a large model that might strain memory\n",
        "class LargeModel(nn.Module):\n",
        "    def __init__(self, layers=50):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(100, 100),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, use_checkpoint=False):\n",
        "        for layer in self.layers:\n",
        "            if use_checkpoint:\n",
        "                x = checkpoint(layer, x)  # With checkpointing\n",
        "            else:\n",
        "                x = layer(x)  # Normal forward pass\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "IKMG1sDOw56B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_measure(model, use_checkpoint):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Track memory usage\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(10):\n",
        "        # Generate random batch\n",
        "        x = torch.randn(32, 100)\n",
        "        y = torch.randn(32, 100)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(x, use_checkpoint=use_checkpoint)\n",
        "        loss = nn.MSELoss()(output, y)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    end_time = time.time()\n",
        "    peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
        "\n",
        "    return end_time - start_time, peak_memory"
      ],
      "metadata": {
        "id": "f0GKOQiHw51m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_methods():\n",
        "    # Initialize model\n",
        "    model = LargeModel()\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Train without checkpointing\n",
        "    time_normal, memory_normal = train_and_measure(model, use_checkpoint=False)\n",
        "\n",
        "    # Train with checkpointing\n",
        "    time_checkpoint, memory_checkpoint = train_and_measure(model, use_checkpoint=True)\n",
        "\n",
        "    # Plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # Memory usage comparison\n",
        "    ax1.bar(['Normal', 'With Checkpointing'],\n",
        "            [memory_normal, memory_checkpoint])\n",
        "    ax1.set_title('Peak Memory Usage (MB)')\n",
        "    ax1.set_ylabel('Memory (MB)')\n",
        "\n",
        "    # Time comparison\n",
        "    ax2.bar(['Normal', 'With Checkpointing'],\n",
        "            [time_normal, time_checkpoint])\n",
        "    ax2.set_title('Training Time (seconds)')\n",
        "    ax2.set_ylabel('Time (s)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"Normal - Time: {time_normal:.2f}s, Memory: {memory_normal:.1f}MB\")\n",
        "    print(f\"Checkpointing - Time: {time_checkpoint:.2f}s, Memory: {memory_checkpoint:.1f}MB\")"
      ],
      "metadata": {
        "id": "KFz9bg0yw5w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    compare_methods()"
      ],
      "metadata": {
        "id": "6oCl85rdw5rM",
        "outputId": "c333df07-ed2e-4c5c-f43c-524ff471af71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'time' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3685183049>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcompare_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4106225016>\u001b[0m in \u001b[0;36mcompare_methods\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Train without checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_normal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train with checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-1240634895>\u001b[0m in \u001b[0;36mtrain_and_measure\u001b[0;34m(model, use_checkpoint)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Track memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_peak_memory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Takeaway\n",
        "\n",
        "*   It saves memory by not storing all intermediate activations\n",
        "*   it recomputes them during the backward pass\n",
        "*   This is a tradeoff with computation time for reduced memory usage\n",
        "*   Particularly useful for training very large models that wouldn't fit in memory"
      ],
      "metadata": {
        "id": "95eY8jckzqeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-11"
      ],
      "metadata": {
        "id": "WH2ISir4R1iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leverage *torch.nn.ModuleList* for Dynamic Model Architectures**  \n",
        "**Dynamically add or remove layers:**\n",
        "*    This is useful for models like recurrent neural networks where the\n",
        "sequence length might vary.\n",
        "\n",
        "**Iterate and modify layers:**  \n",
        "*    Easily loop through and modify layers, e.g., to freeze certain layers or apply different learning rates.\n"
      ],
      "metadata": {
        "id": "oEsSHghJR_Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use case:**  \n",
        "Let us consider a text classification task where the input is a variable list sequence of words and the output is a probability distribution over a set of classes. We can use RNN but the sequence length is variable. Here we can use ModuleList.\n",
        "\n",
        "**How to use ModuleList to make the RNN dynamic:**\n",
        "\n",
        "Variable Number of Layers:\n",
        "Create a ModuleList to store RNN layers.\n",
        "Dynamically add or remove layers based on a hyperparameter or input sequence length.\n",
        "\n",
        "Another Simple Example: Dynamic Linear Regression\n",
        "\n",
        "Let's create a simple linear regression model where we can dynamically add or remove layers. This can be useful for experimenting with different model architectures or for early stopping.\n"
      ],
      "metadata": {
        "id": "8DSQ2IWmeBAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is abstract code snippet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DynamicModel(nn.Module):\n",
        "    def __init__(self): #constructor\n",
        "    \"\"\"\n",
        "        DynamicModel: This is the name of the current class.\n",
        "        self: This refers to the current instance of the DynamicModel class.\n",
        "        By passing these arguments to super(), we're essentially telling Python\n",
        "        to look for the parent class of DynamicModel. This ensures that the\n",
        "        DynamicModel class is properly initialized. Inheritance property.\n",
        "    \"\"\"\n",
        "        super(DynamicModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "a3vgRkD8TmXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another Simple Example: Dynamic Linear Regression**\n",
        "\n",
        "Let's create a simple linear regression model where we can dynamically add or remove layers. This can be useful for experimenting with different model architectures or for early stopping."
      ],
      "metadata": {
        "id": "RWEmGc0ziPNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a basic example, but it demonstrates the flexibility of ModuleList\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample data\n",
        "X = torch.randn(100, 10)  # 100 samples, each with 10 features\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] + torch.randn(100)  # Linear relationship with noise\n",
        "\n",
        "class DynamicLinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DynamicLinearRegression, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_size, 64))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Linear(64, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "model = DynamicLinearRegression(input_size=10, output_size=1)\n",
        "\n",
        "# Add another layer:\n",
        "model.layers.append(nn.Linear(64, 32))\n",
        "model.layers.append(nn.ReLU())\n",
        "\n",
        "# Remove the last layer:\n",
        "del model.layers[-2:]\n"
      ],
      "metadata": {
        "id": "y5S3BKVWiNc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "fi-mDC2ymHP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    output = model(X)\n",
        "    loss = criterion(output, y.unsqueeze(1))  # Reshape y to match output shape\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "E4ltiv9FmIfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "By using a ModuleList, we can easily modify the architecture of the model by adding or removing layers without changing the core forward method. This flexibility is crucial for experimentation and optimization."
      ],
      "metadata": {
        "id": "-sJNLuXbp1Sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch Tip-10\n",
        "Using torch.cuda.amp module\n",
        "\n",
        "**Why Use Mixed Precision?**  \n",
        "Mixed precision training speeds up computations by using lower precision (e.g., float16) while maintaining the accuracy of float32 for critical operations. It reduces memory usage and can accelerate training significantly, especially on GPUs with Tensor Cores like NVIDIAâ€™s.  \n",
        "\n",
        "**How to Implement Mixed Precision Training**\n",
        "Use the torch.cuda.amp (Automatic Mixed Precision) module for seamless integration.\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "model = YourModel().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "scaler = GradScaler()  # Initialize GradScaler for mixed precision\n",
        "\n",
        "for inputs, targets in dataloader:  # Training loop\n",
        "    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Enable mixed precision for the forward pass\n",
        "    with autocast():\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "    \n",
        "    # Scale the loss for backpropagation\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "print(\"Training completed with mixed precision!\")\n",
        "\n",
        "**Benefits:**\n",
        "**Speed:**\n",
        "Mixed precision uses hardware acceleration, reducing training time.\n",
        "Memory Efficiency: Enables larger batch sizes by cutting memory usage.\n",
        "\n",
        "**When to Use:**\n",
        "Ideal for deep learning models that require heavy GPU resources.\n",
        "Works well for architectures like CNNs and Transformers.\n",
        "\n",
        "#PyTorch Tip-9\n",
        "**Gradient Clipping to Prevent Exploding Gradients:**  \n",
        "When training deep neural networks, especially RNNs or LSTMs, gradients can sometimes grow too large, leading to exploding gradients. This causes instability and poor model convergence. To mitigate this, you can use gradient clipping, which limits the size of gradients during backpropagation.  \n",
        "\n",
        "**How to Implement Gradient Clipping:**  \n",
        "PyTorch makes it easy to clip gradients using torch.nn.utils.clip_grad_norm_. This function clips gradients to a maximum specified norm, preventing them from exceeding a certain value.\n",
        "\n",
        "\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "max_norm = 1.0  # Maximum norm for the gradients\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    \n",
        "    loss.backward()  # Backpropagate\n",
        "    \n",
        "    # Clip gradients to avoid exploding gradients\n",
        "    utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "    \n",
        "    optimizer.step()  # Update weights\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "**I used many cases, it helps:**\n",
        "**Stabilizes Training:**  \n",
        "Especially useful when training deep or recurrent networks.\n",
        "**Prevents Gradient Explosion:**  \n",
        "Keeps gradient values in a reasonable range to ensure smooth updates.\n",
        "\n",
        "**Improves Convergence:**\n",
        "Helps models converge more reliably by avoiding runaway updates.\n",
        "\n",
        "Note that this technique is especially useful when working with deep architectures, RNNs, or complex optimization landscapes.\n",
        "\n",
        "#PyTorch Tip-8\n",
        "\n",
        "Diffrence between torch.manual_seed() and\n",
        "torch.cuda.manual_seed() function  \n",
        "**torch.manual_seed()**  \n",
        "This function sets the random seed for generating random numbers for the CPU and all GPU devices in PyTorch. This includes operations like random initialization of tensors, weights, and data augmentation that rely on random number generation.\n",
        "\n",
        "**Effect:**\n",
        "\n",
        "Ensures that any operation involving randomness on the CPU will produce the same result every time the code is run. When you want to ensure consistent random behavior across both CPU and GPU operations in your PyTorch model.  \n",
        "**torch.cuda.manual_seed(42)**  \n",
        " This function sets the random seed specifically for CUDA operations on the current GPU device only. **Does not affect CPU random number generation**  \n",
        " **Use Case:** When you need to ensure consistent random behavior specifically for GPU computations but don't want to affect CPU operations.  \n",
        " ## Key Differences:  \n",
        " | Features |torch.manual_seed(42)|torch.cuda.manual_seed(42)|\n",
        " | :- | -: | :-: |\n",
        " |Scope|Affects both CPU and GPU operations|Affects only the current GPU device|\n",
        " |CPU Randomness|Sets the seed for CPU random number generation|Does not affect CPU random number generation|\n",
        " |GPU Randomness|Sets the seed for all GPU devices|Sets the seed only for the current GPU|\n",
        " |Multi-GPU Behavior|Seed is applied to all available GPUs|Affects only the active GPU (no effect on other GPUs)|\n",
        " |Common Use Case|Reproducibility across both CPU and GPU operations|Reproducibility for operations on the current GPU device only|\n"
      ],
      "metadata": {
        "id": "VXEvHeXNcXm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "yjndNRNncorX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    '''\n",
        "        Class to define the neural network using Linear layers. Importing nn.Module is necessary whenever building any NN\n",
        "    '''\n",
        "\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.layer1 = nn.Linear(in_features=1, out_features=1, bias=True, dtype=torch.float32)\n",
        "        self.layer2 = nn.Linear(in_features=1, out_features=1, bias=True, dtype=torch.float32)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        self.forward1 = self.layer1(x)\n",
        "        return self.layer2(self.forward1)"
      ],
      "metadata": {
        "id": "7Q9oQLz5dN1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.manual_seed(42) #remove comment to see results\n",
        "torch.cuda.manual_seed(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('device = ', device )\n",
        "model_linear = LinearRegression()\n",
        "model_linear.to(device=device)\n",
        "print(model_linear.state_dict())"
      ],
      "metadata": {
        "id": "yoXtKBqKcOTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Tip-7**"
      ],
      "metadata": {
        "id": "Pnko32p-opqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Accumulation for Large Batch Training:**  \n",
        "When training deep learning models on limited hardware (like a single GPU with limited memory), you may not be able to fit a large batch size into memory. Gradient accumulation is a trick that lets you simulate a larger batch size without increasing memory usage.\n",
        "It works using divide and conquer strategy. Instead of processing the entire batch at once, it's divided into smaller sub-batches.\n",
        "After processing all sub-batches, the accumulated gradients are used to update the model's weights.\n",
        "\n",
        "**How to Do It:**  \n",
        "You can accumulate gradients over several smaller batches and update the model only after accumulating enough gradients to match the desired larger batch size.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "GdoXzTymo6bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "DiqAI2fGzfXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model, loss function, and optimizer\n",
        "model = YourModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64  # Desired effective batch size\n",
        "sub_batch_size = 16\n",
        "num_batches = batch_size // sub_batch_size"
      ],
      "metadata": {
        "id": "IarDqwkIzmGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume we want to use an effective batch size of 64, but can only use 16 due to memory constraints\n",
        "accumulation_steps = 4  # Accumulate gradients over 4 small batches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(data_loader):\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss = loss / accumulation_steps  # Scale loss\n",
        "\n",
        "        loss.backward()  # Accumulate gradients\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()  # Update weights\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "# This simulates a larger batch size of 64 by using 4 batches of 16 and accumulating gradients.\n"
      ],
      "metadata": {
        "id": "Taw7WVc_uT3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why This Helps:**  \n",
        "**Memory-efficient:**  \n",
        "You can work with smaller batches while achieving the same effect as\n",
        "training with a larger batch.\n",
        "\n",
        "**Stable training:**  \n",
        " Larger batch sizes can help stabilize gradient updates and potentially lead to better model convergence.\n",
        "\n",
        "This is particularly useful when dealing with large models or datasets where memory is a constraint."
      ],
      "metadata": {
        "id": "EFHWCPuyw-Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gg8-LsK7pBcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **pytorch tip-6**"
      ],
      "metadata": {
        "id": "6gjTHo3wsr0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training a model, efficient data loading is crucial. PyTorch provides the DataLoader class, which can handle batching, shuffling, and parallel data loading with ease.\n",
        "\n",
        "**Benefits of DataLoader:**   \n",
        "Batching: Automatically splits your dataset into batches.\n",
        "Shuffling: Randomizes the order of data, which helps in breaking any potential patterns in the data.\n",
        "Parallel Loading: Loads data in parallel using multiple worker processes, speeding up the data pipeline.  \n",
        "**Example:**  \n",
        "**Step 1: Create a Dataset**  \n",
        "First, you need to create a custom dataset by subclassing torch.utils.data.Dataset.  \n",
        "\n"
      ],
      "metadata": {
        "id": "zYXWPyTnvXB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y\n",
        "\n",
        "# Example data\n",
        "data = torch.randn(1000, 10)\n",
        "labels = torch.randint(0, 2, (1000,))"
      ],
      "metadata": {
        "id": "nU8ZZ6Kz4_dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Create a DataLoader**  \n",
        "Then, you can create a DataLoader to handle batching and shuffling."
      ],
      "metadata": {
        "id": "g7E02s2B5oKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset instance\n",
        "dataset = CustomDataset(data, labels)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Iterate through DataLoader\n",
        "for batch_data, batch_labels in dataloader:\n",
        "    # Your training code here\n",
        "    print(batch_data, batch_labels)\n",
        "    break"
      ],
      "metadata": {
        "id": "LuBxjHgf4Swr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **pytorch tip-5**"
      ],
      "metadata": {
        "id": "-ZUqHCSN3Jle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A PyTorch tip that combines efficiency and debugging best practices:\n",
        "\n",
        "**Overfit a Single Batch for Sanity Checks**\n",
        "\n",
        "Before investing significant time training on a large dataset, use a small batch to verify your model's functionality. This can catch errors early and save you time:\n",
        "\n",
        "**Create a Data Loader:** Set up your data loader as usual for training.  \n",
        "**Grab a Single Batch:** Extract the first batch of data (images, labels) from the data loader using next(iter(data_loader)).  \n",
        "**Overfit the Batch:** Train your model on this single batch for a few epochs. You expect the model to overfit (achieve very high accuracy) on this small sample."
      ],
      "metadata": {
        "id": "9rsi_4Zo3cm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have your model (`model`) and data loader (`data_loader`) defined\n",
        "\n",
        "# Get the first batch of data\n",
        "images, labels = next(iter(data_loader))\n",
        "\n",
        "# Train on the single batch for a few epochs (e.g., 5 epochs)\n",
        "    # ... your training loop logic using `images` and `labels` ...\n",
        "for epoch in range(5):\n",
        "    for batch_images, batch_labels in data_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_images)\n",
        "        loss = loss_function(outputs, batch_labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "# Evaluate the model's performance on the single batch (optional)\n",
        "# You can calculate metrics such as accuracy, precision, recall, etc.\n",
        "\n",
        "#if your model cannot even overfit a single batch, it suggests an issue with the\n",
        "#model architecture, learning process, or data preprocessing.\n",
        "\n",
        "# If successful, proceed with training on the full dataset\n"
      ],
      "metadata": {
        "id": "bqL0n4OxxYay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits:**\n",
        "\n",
        "**Early Debugging:** Catch potential errors before extensive training.  \n",
        "**Faster Iteration:** Quickly test model changes without waiting for full dataset training.  \n",
        "**Efficiency:** Reduce training time on large datasets if there are fundamental issues.  \n",
        "\n",
        "**Remember:** Overfitting a single batch is a sanity check, not a complete evaluation. If successful, proceed with training on the entire dataset."
      ],
      "metadata": {
        "id": "_VOsZd7Cxg4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dEjIW3Moc9_"
      },
      "outputs": [],
      "source": [
        "# prompt: pytorch tips\n",
        "\n",
        "# Use `torch.cuda.is_available()` to check if CUDA is available before using it.\n",
        "# This can help avoid errors and improve performance.\n",
        "\n",
        "# Use `torch.no_grad()` context manager to disable gradient computation when it is not needed.\n",
        "# This can save memory and improve performance.\n",
        "\n",
        "# Use `torch.jit.trace()` to create a traced version of your model for improved performance.\n",
        "# This can be especially useful for models that are used frequently.\n",
        "\n",
        "# Use `torch.utils.data.DataLoader` to load your data in batches.\n",
        "# This can help improve performance by reducing the number of times the data is loaded into memory.\n",
        "\n",
        "# Use `torch.optim.lr_scheduler` to adjust the learning rate during training.\n",
        "# This can help improve the convergence of the model.\n",
        "\n",
        "# Use `torch.nn.utils.clip_grad_norm_` to clip the gradients during training.\n",
        "# This can help prevent the gradients from becoming too large and causing the model to diverge.\n",
        "\n",
        "# Use `torch.utils.tensorboard` to visualize the training process.\n",
        "# This can help you track the progress of the model and identify any potential problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Tip - 4**"
      ],
      "metadata": {
        "id": "tDaNebn9-SSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore Static Graphs with torch.compile (PyTorch 2.0 or later):**\n",
        "\n",
        "If you're using PyTorch version 2.0 or above and aiming to deploy your model for inference, consider leveraging **torch.compile**. This feature offers significant speedups by converting your model's dynamic computational graph into a static one. **torch.compile** makes PyTorch code run faster by JIT-compiling PyTorch code into optimized kernel.\n",
        "\n",
        "**Understanding Dynamic vs. Static Graphs:**\n",
        "\n",
        "**Dynamic Graphs (Default):**In PyTorch's eager execution mode, the computational graph is built on-the-fly during each forward pass. While flexible, this approach can introduce overhead due to graph creation in every run.\n",
        "**Static Graphs**: torch.compile optimizes the model by pre-compiling the computational graph into a more efficient, fixed structure. This static graph can then be repeatedly executed for inference tasks, leading to faster predictions.\n"
      ],
      "metadata": {
        "id": "vgxKvdIm-uxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trade-offs:** While torch.compile generally accelerates inference, it might incur a slight overhead during the compilation process itself. However, this is usually a one-time cost that outweighs the benefits in most deployment scenarios.  \n",
        "**Limited Flexibility:** Once compiled, the static graph cannot be easily modified. If your model needs dynamic adjustments at runtime, torch.compile might not be the most suitable option."
      ],
      "metadata": {
        "id": "RsY3dnN7AMQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Use torch.compile:**"
      ],
      "metadata": {
        "id": "XcfyFT5KAcNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load your trained model\n",
        "\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "# Use the compiled model for inference on new data\n",
        "predictions = compiled_model(data)\n"
      ],
      "metadata": {
        "id": "Bs_bUVuZAhv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python functions can be optimized by passing the callable to torch.compile. We can then call the returned optimized function in place of the original function."
      ],
      "metadata": {
        "id": "cRMvv6HcCZ20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def foo(x, y):\n",
        "    a = torch.sin(x)\n",
        "    b = torch.cos(y)\n",
        "    return a + b\n",
        "opt_foo1 = torch.compile(foo)\n",
        "print(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))"
      ],
      "metadata": {
        "id": "DqyqIfbyCb-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can decorate the function."
      ],
      "metadata": {
        "id": "ayD8qbdbCo21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.compile\n",
        "def opt_foo2(x, y):\n",
        "    a = torch.sin(x)\n",
        "    b = torch.cos(y)\n",
        "    return a + b\n",
        "print(opt_foo2(torch.randn(10, 10), torch.randn(10, 10)))"
      ],
      "metadata": {
        "id": "gQcFqfwPCp0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits of torch.compile:**\n",
        "\n",
        "**Reduced Inference Latency:** By eliminating the need to construct the graph dynamically each time, you can achieve noticeably faster inference speeds. This is crucial for real-time applications where low latency is essential.  \n",
        "**Potential for Further Optimizations:** torch.compile often paves the way for additional optimizations under the hood, such as kernel fusion* and improved memory access patterns.  \n",
        "**kernel fusion** is a valuable technique for optimizing code running on GPUs. By reducing data transfer overhead and improving cache utilization, it can significantly accelerate computations.  \n",
        "\n",
        "By adopting torch.compile for deployment, you can significantly enhance your model's inference performance, making it more efficient and responsive in real-world applications.\n",
        "\n",
        "Further Reading: https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"
      ],
      "metadata": {
        "id": "yyFCdjiCAor0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Tip - 3**\n"
      ],
      "metadata": {
        "id": "uh9hzytOo3BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using torch.where() for Conditional Element-wise Operations**\n",
        "PyTorch's **torch.where()** function allows you to perform conditional element-wise operations efficiently. It takes three arguments: the condition, the tensor to select values from when the condition is true, and the tensor to select values from when the condition is false. This is particularly useful for implementing conditional logic within your neural network models.\n",
        "\n",
        "Here's a quick example:\n",
        "In this example, the elements from tensor_true are selected where the condition is True, and elements from tensor_false are selected where the condition is False. This allows for flexible conditional operations within your PyTorch code."
      ],
      "metadata": {
        "id": "Jcg0XpAApdau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define tensors\n",
        "condition = torch.tensor([[True, False], [False, True]])\n",
        "tensor_true = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor_false = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Perform conditional element-wise operation\n",
        "result = torch.where(condition, tensor_true, tensor_false)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "m7mRti28pMqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some Practical usages below:**"
      ],
      "metadata": {
        "id": "qzSlhc4ivTZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Masked Operations:** You might have a tensor and want to perform different operations on elements based on some condition. For instance, in natural language processing, you might want to mask out certain tokens during tokenization or in attention mechanisms based on some condition."
      ],
      "metadata": {
        "id": "UvnyYYzhttJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example: Masking tokens with a special token ID\n",
        "input_ids = torch.tensor([101, 102, 103, 104, 105])  # Example input tensor\n",
        "mask_condition = input_ids == 103  # Condition to mask out token with ID 103\n",
        "special_token_id = 1000  # Special token ID to replace masked tokens\n",
        "\n",
        "# Mask out tokens with ID 103\n",
        "masked_input_ids = torch.where(mask_condition, torch.tensor(special_token_id), input_ids)\n",
        "\n",
        "print(masked_input_ids)\n"
      ],
      "metadata": {
        "id": "wMQbnWpgueW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Loss Function Modification:** During training, you may want to apply different weights to different elements of the loss function based on some condition."
      ],
      "metadata": {
        "id": "LftkpiTduzQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Example: Modifying loss function based on class imbalance\n",
        "predicted_scores = torch.tensor([0.1, 0.8, 0.3, 0.9, 0.2])  # Example predicted scores\n",
        "true_labels = torch.tensor([0, 1, 0, 1, 1])  # Example true labels\n",
        "\n",
        "# Calculate binary cross-entropy loss with class imbalance handling\n",
        "positive_weight = 2.0  # Weight for positive class\n",
        "negative_weight = 1.0  # Weight for negative class\n",
        "loss_weights = torch.where(true_labels == 1, positive_weight, negative_weight)\n",
        "\n",
        "# Calculate weighted binary cross-entropy loss\n",
        "loss = F.binary_cross_entropy_with_logits(predicted_scores, true_labels.float(), weight=loss_weights)\n",
        "\n",
        "print(loss)\n"
      ],
      "metadata": {
        "id": "SJLbatNqu_dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Interpretability:** In some scenarios, you might want to interpret the output of your model differently based on certain conditions."
      ],
      "metadata": {
        "id": "9Kravs2713ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example: Interpreting model output differently based on confidence\n",
        "output_scores = torch.tensor([0.8, 0.6, 0.9, 0.4, 0.7])  # Example output scores\n",
        "confidence_threshold = 0.7  # Threshold for high confidence\n",
        "\n",
        "# Determine model predictions based on confidence\n",
        "predictions = torch.where(output_scores >= confidence_threshold, torch.tensor(1), torch.tensor(0))\n",
        "\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "awdjw-uq16Ft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}